{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "+ One way Analysis of Variance (ANOVA)\n",
    "+ Kruskal Wallis test\n",
    "+ Chi Square Goodness of Fit test\n",
    "+ Chi Square Tests for Independence / Association Test"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: researchpy in /Users/tnathu-ai/.conda/envs/Advanced_Programming_for_Data_Science/lib/python3.10/site-packages (0.3.5)\r\n",
      "Requirement already satisfied: patsy in /Users/tnathu-ai/.conda/envs/Advanced_Programming_for_Data_Science/lib/python3.10/site-packages (from researchpy) (0.5.2)\r\n",
      "Requirement already satisfied: pandas in /Users/tnathu-ai/.conda/envs/Advanced_Programming_for_Data_Science/lib/python3.10/site-packages (from researchpy) (1.4.3)\r\n",
      "Requirement already satisfied: statsmodels in /Users/tnathu-ai/.conda/envs/Advanced_Programming_for_Data_Science/lib/python3.10/site-packages (from researchpy) (0.13.2)\r\n",
      "Requirement already satisfied: scipy in /Users/tnathu-ai/.conda/envs/Advanced_Programming_for_Data_Science/lib/python3.10/site-packages (from researchpy) (1.9.0)\r\n",
      "Requirement already satisfied: numpy in /Users/tnathu-ai/.conda/envs/Advanced_Programming_for_Data_Science/lib/python3.10/site-packages (from researchpy) (1.23.1)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /Users/tnathu-ai/.conda/envs/Advanced_Programming_for_Data_Science/lib/python3.10/site-packages (from pandas->researchpy) (2.8.2)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/tnathu-ai/.conda/envs/Advanced_Programming_for_Data_Science/lib/python3.10/site-packages (from pandas->researchpy) (2022.1)\r\n",
      "Requirement already satisfied: six in /Users/tnathu-ai/.conda/envs/Advanced_Programming_for_Data_Science/lib/python3.10/site-packages (from patsy->researchpy) (1.16.0)\r\n",
      "Requirement already satisfied: packaging>=21.3 in /Users/tnathu-ai/.conda/envs/Advanced_Programming_for_Data_Science/lib/python3.10/site-packages (from statsmodels->researchpy) (21.3)\r\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/tnathu-ai/.conda/envs/Advanced_Programming_for_Data_Science/lib/python3.10/site-packages (from packaging>=21.3->statsmodels->researchpy) (3.0.4)\r\n"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "!pip install researchpy\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import researchpy as rp\n",
    "from scipy.stats import stats"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numeric columns:  ['athlete_id', 'age', 'height', 'weight', 'fran', 'helen', 'grace', 'filthy50', 'fgonebad', 'run400', 'run5k', 'candj', 'snatch', 'deadlift', 'backsq', 'pullups', 'rank', 'score'] \n",
      "\n",
      "String columns:  ['name', 'region', 'team', 'affiliate', 'gender', 'eat', 'train', 'background', 'experience', 'schedule', 'division'] \n",
      "\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 991 entries, 0 to 990\n",
      "Data columns (total 29 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   athlete_id  991 non-null    Int64 \n",
      " 1   name        991 non-null    string\n",
      " 2   region      991 non-null    string\n",
      " 3   team        991 non-null    string\n",
      " 4   affiliate   991 non-null    string\n",
      " 5   gender      991 non-null    string\n",
      " 6   age         991 non-null    Int64 \n",
      " 7   height      991 non-null    Int64 \n",
      " 8   weight      991 non-null    Int64 \n",
      " 9   fran        991 non-null    Int64 \n",
      " 10  helen       991 non-null    Int64 \n",
      " 11  grace       991 non-null    Int64 \n",
      " 12  filthy50    991 non-null    Int64 \n",
      " 13  fgonebad    991 non-null    Int64 \n",
      " 14  run400      991 non-null    Int64 \n",
      " 15  run5k       991 non-null    Int64 \n",
      " 16  candj       991 non-null    Int64 \n",
      " 17  snatch      991 non-null    Int64 \n",
      " 18  deadlift    991 non-null    Int64 \n",
      " 19  backsq      991 non-null    Int64 \n",
      " 20  pullups     991 non-null    Int64 \n",
      " 21  eat         991 non-null    string\n",
      " 22  train       991 non-null    string\n",
      " 23  background  991 non-null    string\n",
      " 24  experience  991 non-null    string\n",
      " 25  schedule    991 non-null    string\n",
      " 26  division    991 non-null    string\n",
      " 27  rank        991 non-null    Int64 \n",
      " 28  score       991 non-null    Int64 \n",
      "dtypes: Int64(18), string(11)\n",
      "memory usage: 242.1 KB\n",
      "The shape and df type of the ORGINAL df: None\n"
     ]
    },
    {
     "data": {
      "text/plain": "   athlete_id              name         region                team  \\\n0        2720      justin adams  south central   woodward crossfit   \n1        6922    daniel adamson     south west  crossfit the point   \n2       12563  steven lee adams   mid atlantic      crossfit kaiju   \n\n            affiliate gender  age  height  weight  fran  ...  backsq  pullups  \\\n0   woodward crossfit   male   24      68     180   126  ...     405       80   \n1  crossfit the point   male   31      67     150   244  ...     330       42   \n2      crossfit kaiju   male   37      72     210   162  ...     425       49   \n\n                                                 eat  \\\n0  i eat quality foods but don't measure the amount|   \n1  i eat quality foods but don't measure the amount|   \n2  i eat quality foods but don't measure the amou...   \n\n                                               train  \\\n0  i workout mostly at a crossfit affiliate|i hav...   \n1  i workout mostly at a crossfit affiliate|i inc...   \n2  i workout mostly at a crossfit affiliate|i inc...   \n\n                                          background  \\\n0        i played youth or high school level sports|   \n1                           i played college sports|   \n2  i played youth or high school level sports|i p...   \n\n                                          experience  \\\n0  i began crossfit with a coach (e.g. at an affi...   \n1  i began crossfit by trying it alone (without a...   \n2  i began crossfit with a coach (e.g. at an affi...   \n\n                                            schedule  division   rank  score  \n0  i do multiple workouts in a day 3+ times a wee...      male   3448    464  \n1  i usually only do 1 workout a day|i do multipl...      male  35748    712  \n2  i do multiple workouts in a day 2x a week|i ty...      male   5073    485  \n\n[3 rows x 29 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>athlete_id</th>\n      <th>name</th>\n      <th>region</th>\n      <th>team</th>\n      <th>affiliate</th>\n      <th>gender</th>\n      <th>age</th>\n      <th>height</th>\n      <th>weight</th>\n      <th>fran</th>\n      <th>...</th>\n      <th>backsq</th>\n      <th>pullups</th>\n      <th>eat</th>\n      <th>train</th>\n      <th>background</th>\n      <th>experience</th>\n      <th>schedule</th>\n      <th>division</th>\n      <th>rank</th>\n      <th>score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2720</td>\n      <td>justin adams</td>\n      <td>south central</td>\n      <td>woodward crossfit</td>\n      <td>woodward crossfit</td>\n      <td>male</td>\n      <td>24</td>\n      <td>68</td>\n      <td>180</td>\n      <td>126</td>\n      <td>...</td>\n      <td>405</td>\n      <td>80</td>\n      <td>i eat quality foods but don't measure the amount|</td>\n      <td>i workout mostly at a crossfit affiliate|i hav...</td>\n      <td>i played youth or high school level sports|</td>\n      <td>i began crossfit with a coach (e.g. at an affi...</td>\n      <td>i do multiple workouts in a day 3+ times a wee...</td>\n      <td>male</td>\n      <td>3448</td>\n      <td>464</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>6922</td>\n      <td>daniel adamson</td>\n      <td>south west</td>\n      <td>crossfit the point</td>\n      <td>crossfit the point</td>\n      <td>male</td>\n      <td>31</td>\n      <td>67</td>\n      <td>150</td>\n      <td>244</td>\n      <td>...</td>\n      <td>330</td>\n      <td>42</td>\n      <td>i eat quality foods but don't measure the amount|</td>\n      <td>i workout mostly at a crossfit affiliate|i inc...</td>\n      <td>i played college sports|</td>\n      <td>i began crossfit by trying it alone (without a...</td>\n      <td>i usually only do 1 workout a day|i do multipl...</td>\n      <td>male</td>\n      <td>35748</td>\n      <td>712</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>12563</td>\n      <td>steven lee adams</td>\n      <td>mid atlantic</td>\n      <td>crossfit kaiju</td>\n      <td>crossfit kaiju</td>\n      <td>male</td>\n      <td>37</td>\n      <td>72</td>\n      <td>210</td>\n      <td>162</td>\n      <td>...</td>\n      <td>425</td>\n      <td>49</td>\n      <td>i eat quality foods but don't measure the amou...</td>\n      <td>i workout mostly at a crossfit affiliate|i inc...</td>\n      <td>i played youth or high school level sports|i p...</td>\n      <td>i began crossfit with a coach (e.g. at an affi...</td>\n      <td>i do multiple workouts in a day 2x a week|i ty...</td>\n      <td>male</td>\n      <td>5073</td>\n      <td>485</td>\n    </tr>\n  </tbody>\n</table>\n<p>3 rows × 29 columns</p>\n</div>"
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set the general path of the external df\n",
    "external_df_path = os.path.join(os.path.pardir, 'data', 'interim')\n",
    "\n",
    "# set the path for specific dfset from external dfset\n",
    "df = os.path.join(external_df_path, 'cleaned_data.csv')\n",
    "\n",
    "# import dfset\n",
    "df = pd.read_csv(df, delimiter=',', skipinitialspace=True)\n",
    "\n",
    "# convert columns to the best possible dtypes, object->string\n",
    "df = df.convert_dtypes()\n",
    "\n",
    "# select numeric columns\n",
    "df_numeric = df.select_dtypes(include=[np.number]).columns.to_list()\n",
    "\n",
    "# select non-numeric columns\n",
    "df_string = df.select_dtypes(include='string').columns.tolist()\n",
    "\n",
    "print(\"Numeric columns: \", df_numeric, \"\\n\")\n",
    "print(\"String columns: \", df_string, \"\\n\\n\")\n",
    "\n",
    "# print dfset info\n",
    "print(\"The shape and df type of the ORGINAL df:\", str(df.info()))\n",
    "\n",
    "# print first 3 rows\n",
    "df.head(3)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# HOMOGENEITY OF VARIANCE\n",
    "\n",
    "testing this assumption is the Levene's test of homogeneity of variances. This can be completed using the levene() method from Scipy.stats."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "# stats.levene(*[df['score'][df['region'] == region] for region in df.region.unique()])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## -------> OBSERVATION\n",
    "The Levene's test of homogeneity of variances is not significant which indicates that the groups have non-statistically significant difference in their varability. Again, it may be worthwhile to check this assumption visually as well."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### ------> OBSERVATIONS\n",
    "The graphical testing of homogeneity of variances supports the statistical testing findings which is the groups have equal variance.\n",
    "\n",
    "By default box plots show the median (orange line in graph above). The green triangle is the mean for each group which was an additional argument that was passed into the method."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h1 style=\"color:#ffc0cb;font-size:40px;font-family:Georgia;text-align:center;\"><strong>One way Analysis of Variance (ANOVA)</strong></h1>\n",
    "\n",
    "## ANOVA Hypotheses\n",
    "+ Null hypothesis: Groups means are equal (no variation in means of groups)\n",
    "H0: μ1=μ2=…=μp\n",
    "+ Alternative hypothesis: At least, one group mean is different from other groups\n",
    "H1: All μ are not equal\n",
    "\n",
    "### Parametric test assumptions\n",
    "+ Population distributions are normal\n",
    "+ Samples have equal variances\n",
    "+ Independence\n",
    "\n",
    "## ANOVA Assumptions\n",
    "+ Residuals (experimental error) are approximately normally distributed (Shapiro-Wilks test or histogram)homoscedasticity or Homogeneity of variances (variances are equal between treatment groups) (Levene’s, Bartlett’s, or Brown-Forsythe test)\n",
    "+ Observations are sampled independently from each other (no relation in observations between the groups and within the groups) i.e., each subject should have only one response\n",
    "+ The dependent variable should be continuous. If the dependent variable is ordinal or rank (e.g. Likert item data), it is more likely to violate the assumptions of normality and homogeneity of variances. If these assumptions are violated, you should consider the non-parametric tests (e.g. Mann-Whitney U test, Kruskal-Wallis test).\n",
    "\n",
    "\n",
    "## How ANOVA works?\n",
    "+ Check sample sizes: equal number of observation in each group\n",
    "+ Calculate Mean Square for each group (MS) (SS of group/level-1); level-1 is a degrees of freedom (df) for a group\n",
    "+ Calculate Mean Square error (MSE) (SS error/df of residuals)\n",
    "+ Calculate F value (MS of group/MSE)\n",
    "+ Calculate p value based on F value and degrees of freedom (df)\n",
    "\n",
    "## Questions?\n",
    "+ Imbalance label problem (unequal sample size for each group) data\n",
    "\n",
    "\n",
    "| **ANOVA Source** | **df** | **SS** | **MS**          | **F**    | **Notes**           |\n",
    "|:----------------:|:------:|:------:|:---------------:|:--------:|:-------------------:|\n",
    "| **Treatments**   |  k-1   | SSTr   | MSTr=SSTr/(k-1) | MSTr/MSE | k: number of groups |\n",
    "| **Errors**       |  n-k   | SSE    | MSE=SSE/(n-k)   |          | n: sample size      |\n",
    "| **Total**        |  n-1   | SST    |                 |          |                     |\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# F Critical Value\n",
    "\n",
    "`scipy.stats.f.ppf(q, dfn, dfd)`\n",
    "\n",
    "where:\n",
    "\n",
    "`q`: The significance level to use\n",
    "`dfn`: The numerator degrees of freedom\n",
    "`dfd`: The denominator degrees of freedom"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "\n",
    "Steps in the test:\n",
    "1. First write down the null and alternate hypothesis.\n",
    "    >> H0 : μ1 =μ2 =μ3 =···=μk\n",
    "    >> H1 : at least one pair have different means.\n",
    "2. Next calculate the test statistic F using the data and present it in a\n",
    "table as above.\n",
    "3. Find the Rejection region for the corresponding alternate hypothesis\n",
    "and chosen α value, that is find Fα,k−1,n−k.\n",
    "4. Reject or Don’t Reject If the test statistic F falls in the rejection region, reject H0 and conclude H1 is true, or else do not reject H0. You should also interpret the result in words."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "F_onewayResult(statistic=1.1889922399442483, pvalue=0.26993810389596873)"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ONE-WAY ANOVA USING SCIPY.STATS\n",
    "\n",
    "# calculate f_oneway by looping through unique regions\n",
    "stats.f_oneway(*[df['score'][df['region'] == region] for region in df.region.unique()])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 1.\n",
    "\n",
    "You want to compare the cleaning action of 7 detergents using a one-way analysis of variance.\n",
    "You cut 31 strips of dirty cloth and randomly assign (wash) each strip with one of the detergents.\n",
    "You then measure their whiteness with a reflectance meter.\n",
    "How many degrees of freedom does the sum of squares total have?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "24\n",
      "F critical value: 2.508188823423255\n"
     ]
    }
   ],
   "source": [
    "import scipy.stats\n",
    "\n",
    "# F0.05,2,12 = 3.89\n",
    "# number of values in all groups\n",
    "n = 31\n",
    "# number of groups\n",
    "k = 7\n",
    "# significance level\n",
    "q = 1 - .05\n",
    "# numerator degrees of freedom\n",
    "dfn = k - 1\n",
    "print(dfn)\n",
    "# denominator degrees of freedom\n",
    "dfd = n - k\n",
    "print(dfd)\n",
    "#find F critical value\n",
    "print(f'F critical value: {scipy.stats.f.ppf(q=q, dfn=dfn, dfd=dfd)}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F critical value: 3.492828476735632\n"
     ]
    }
   ],
   "source": [
    "import scipy.stats\n",
    "\n",
    "# F0.05,2,12 = 3.89\n",
    "\n",
    "\n",
    "performance1 = [8, 9, 11, 10, 11, 9, 10, 10]\n",
    "performance2 = [13, 10, 9, 10, 11, 8, 14, 13]\n",
    "performance3 = [10, 12, 13, 10, 11, 11, 13]\n",
    "\n",
    "# number of values in all groups\n",
    "n = len(performance1) + len(performance2) + len(performance3)\n",
    "# number of groups\n",
    "k = 3\n",
    "# significance level\n",
    "q = 1 - .05\n",
    "# numerator degrees of freedom\n",
    "dfn = k - 1\n",
    "# denominator degrees of freedom\n",
    "dfd = n - k\n",
    "#find F critical value\n",
    "print(f'F critical value: {scipy.stats.f.ppf(q=q, dfn=dfn, dfd=dfd)}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [
    {
     "data": {
      "text/plain": "F_onewayResult(statistic=2.368271597147726, pvalue=0.11937403275881779)"
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing library\n",
    "from scipy.stats import f_oneway\n",
    "\n",
    "performance1 = [8, 9, 11, 10, 11, 9, 10, 10]\n",
    "performance2 = [13, 10, 9, 10, 11, 8, 14, 13]\n",
    "performance3 = [10, 12, 13, 10, 11, 11, 13]\n",
    "\n",
    "# Conduct the one-way ANOVA\n",
    "f_oneway(performance1, performance2, performance3)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### -------> OBSERVATION\n",
    "\n",
    "The purpose of this study was to test for a difference in score between the region. The overall average score was 610.55 95% CI(600.81,620.29) with group averages of.... There is a statistically insignificant difference between the regions and their effects the scores, F= 1.19, p-value= 0.27.\n",
    "\n",
    "As the p value (0.27) is insignificant, we fail to reject the null hypothesis and conclude that regions have equal variances.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h1 style=\"color:#ffc0cb;font-size:40px;font-family:Georgia;text-align:center;\"><strong>Kruskal Wallis test</strong></h1>\n",
    "\n",
    "\n",
    "Assumptions for the test:\n",
    "1. At least one of the two large sample conditions are met.\n",
    "2. All the samples are random samples.\n",
    "3. All the populations being sampled have the same shaped probability density function, with possibly different means.\n",
    "4. The populations are independent.\n",
    "\n",
    "H0 : μ1 =μ2 =μ3 =···=μk\n",
    "H1 : at least two μi differ.\n",
    "\n",
    "**NOTE**: The larger the differences the larger the test statistic H. This is why the test is only an upper tail test."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "degrees of freedom: 3\n",
      "The critical value X²U for the upper tail is 11.344866730144373\n"
     ]
    }
   ],
   "source": [
    "# Find the Chi-Square Critical Value\n",
    "import scipy.stats\n",
    "\n",
    "# find Chi-Square critical value for 2 tail hypothesis tests\n",
    "alpha = float(0.01)\n",
    "k = 4\n",
    "degree_freedom = k - 1\n",
    "print(f'degrees of freedom: {(k - 1)}')\n",
    "# X² for upper tail\n",
    "print(f'The critical value X²U for the upper tail is {scipy.stats.chi2.ppf(1 - alpha, df=degree_freedom)}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KruskalResult(statistic=21.644331786306708, pvalue=0.15505130496906336)\n"
     ]
    }
   ],
   "source": [
    "# Conduct the Kruskal-Wallis Test\n",
    "result = stats.kruskal(*[df['score'][df['region'] == region] for region in df.region.unique()])\n",
    "\n",
    "# Print the result\n",
    "print(result)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [
    {
     "data": {
      "text/plain": "KruskalResult(statistic=2.0390527509926217, pvalue=0.5643408523150142)"
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# group1 = [624, 680, 454, 510, 539]\n",
    "# group2 = [425, 595, 737, 459, 709, 482]\n",
    "# group3 = [397, 794, 595, 539, 680, 652]\n",
    "# group4 = [482, 510, 369, 567, 595]\n",
    "#\n",
    "#\n",
    "# from scipy import stats\n",
    "#\n",
    "# #perform Kruskal-Wallis Test\n",
    "# stats.kruskal(group1, group2, group3, group4)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h1 style=\"color:#ffc0cb;font-size:40px;font-family:Georgia;text-align:center;\"><strong>Chi Square Goodness of Fit test</strong></h1>\n",
    "\n",
    "The Chi-Square Goodness of fit test is a non-parametric statistical hypothesis test that’s used to determine how considerably the observed value of an event differs from the expected value. it helps us check whether a variable comes from a certain distribution or if a sample represents a population. The observed probability distribution is compared with the expected probability distribution.\n",
    "\n",
    "\n",
    "if chi_square_ value > critical value, the null hypothesis is rejected. if chi_square_ value <= critical value, the null hypothesis is accepted.\n",
    "\n",
    "\n",
    "H0: (null hypothesis) A variable follows a hypothesized distribution.\n",
    "H1: (alternative hypothesis) A variable does not follow a hypothesized distribution.\n",
    "\n",
    "`chisquare(f_obs, f_exp)`\n",
    "\n",
    "where:\n",
    "\n",
    "`f_obs`: An array of observed counts.\n",
    "`f_exp`: An array of expected counts. By default, each category is assumed to be equally likely."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "Power_divergenceResult(statistic=4.359999999999999, pvalue=0.3594720674366307)"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First, we will create two arrays to hold our observed and expected number of customers for each day:\n",
    "expected = [50, 50, 50, 50, 50]\n",
    "observed = [50, 60, 40, 47, 53]\n",
    "\n",
    "import scipy.stats as stats\n",
    "\n",
    "#perform Chi-Square Goodness of Fit Test\n",
    "stats.chisquare(f_obs=observed, f_exp=expected)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Note that the p-value corresponds to a Chi-Square value with n-1 degrees of freedom (dof), where n is the number of different categories. In this case, dof = 5-1 = 4. You can use the Chi-Square to P Value Calculator to confirm that the p-value that corresponds to X2 = 4.36 with dof = 4 is 0.35947.\n",
    "\n",
    "Since the p-value (.35947) is not less than 0.05, we fail to reject the null hypothesis. This means we do not have sufficient evidence to say that the true distribution of customers is different from the distribution that the shop owner claimed."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "A genetics experiment, crossing two types of sorghum, in theory, should produce offspring with the colours red, yellow, and white in the ratio 9:3:4.\n",
    "The outcome for 368 experimental plants was 195 red, 73 yellow, and 100 white. Does this data contradict the theory, using α = 0.01?\n",
    "The probabilities are: red, 9/16; yellow, 3/16, and white, 4/16. If we let red be outcome 1, yellow be outcome 2, and white outcome 3, we have the following null and alternate hypothesis."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Degree of freedom is: 3\n",
      "chi_square_test_statistic is : 26.77216117216117\n",
      "p_value : 6.571708865255795e-06\n",
      "11.344866730144373\n"
     ]
    }
   ],
   "source": [
    "# importing packages\n",
    "import scipy.stats as stats\n",
    "\n",
    "# n = 168\n",
    "# p1=0.35\n",
    "# p2=0.35\n",
    "# p3=0.2\n",
    "# p4=0.1\n",
    "# no of hours a student studies\n",
    "# in a week vs expected no of hours\n",
    "observed = [27, 31, 25, 17]\n",
    "# expected = [p1*n, p2*n, p3*n, p4*n]\n",
    "expected = [15, 21, 25, 39]\n",
    "\n",
    "degree_freedom = int(len(observed) - 1)\n",
    "alpha = float(0.01)\n",
    "\n",
    "# Chi-Square Goodness of Fit Test\n",
    "chi_square_test_statistic, p_value = stats.chisquare(\n",
    "    observed, expected)\n",
    "\n",
    "print(f'Degree of freedom is: {degree_freedom}')\n",
    "# chi square test statistic and p value\n",
    "print('chi_square_test_statistic is : ' +\n",
    "      str(chi_square_test_statistic))\n",
    "print('p_value : ' + str(p_value))\n",
    "\n",
    "# find Chi-Square critical value\n",
    "print(stats.chi2.ppf(1 - alpha, df=degree_freedom))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h1 style=\"color:#ffc0cb;font-size:40px;font-family:Georgia;text-align:center;\"><strong>Chi Square Test for Independence</strong></h1>\n",
    "\n",
    "\n",
    "### χ^2 test of independence assumptions\n",
    "+ The two samples are independent\n",
    "+ No expected cell count is = 0\n",
    "+ No more than 20% of the cells have and expected cell count < 5\n",
    "\n",
    "![](../media/images/tests_for_Independence.png)\n",
    "\n",
    "The null and alternate hypothesis in this problem are,\n",
    "H0 : The two factors are independent.\n",
    "H1 : the two factors are dependent."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "degrees of freedom: 4\n",
      "The critical value X²U for the upper tail is 9.487729036781154\n"
     ]
    }
   ],
   "source": [
    "# Find the Chi-Square Critical Value\n",
    "import scipy.stats\n",
    "\n",
    "# find Chi-Square critical value for 2 tail hypothesis tests\n",
    "alpha = float(0.05)\n",
    "rows = 2\n",
    "cols = 5\n",
    "degree_freedom = (rows - 1) * (cols - 1)\n",
    "print(f'degrees of freedom: {(rows - 1) * (cols - 1)}')\n",
    "# X² for upper tail\n",
    "print(f'The critical value X²U for the upper tail is {scipy.stats.chi2.ppf(1 - alpha, df=degree_freedom)}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "   Variable              Outcome  Count  Percent\n0    region        north central    109     11.0\n1                     south east    104    10.49\n2                     north east     89     8.98\n3                     south west     77     7.77\n4                   mid atlantic     77     7.77\n5                  south central     76     7.67\n6            southern california     70     7.06\n7                     north west     65     6.56\n8            northern california     64     6.46\n9                   central east     59     5.95\n10                     australia     53     5.35\n11                        europe     42     4.24\n12                   canada west     40     4.04\n13                   canada east     33     3.33\n14                 latin america     14     1.41\n15                          asia     13     1.31\n16                        africa      6     0.61\n17   gender                 male    813    82.04\n18                        female    178    17.96",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Variable</th>\n      <th>Outcome</th>\n      <th>Count</th>\n      <th>Percent</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>region</td>\n      <td>north central</td>\n      <td>109</td>\n      <td>11.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td></td>\n      <td>south east</td>\n      <td>104</td>\n      <td>10.49</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td></td>\n      <td>north east</td>\n      <td>89</td>\n      <td>8.98</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td></td>\n      <td>south west</td>\n      <td>77</td>\n      <td>7.77</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td></td>\n      <td>mid atlantic</td>\n      <td>77</td>\n      <td>7.77</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td></td>\n      <td>south central</td>\n      <td>76</td>\n      <td>7.67</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td></td>\n      <td>southern california</td>\n      <td>70</td>\n      <td>7.06</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td></td>\n      <td>north west</td>\n      <td>65</td>\n      <td>6.56</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td></td>\n      <td>northern california</td>\n      <td>64</td>\n      <td>6.46</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td></td>\n      <td>central east</td>\n      <td>59</td>\n      <td>5.95</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td></td>\n      <td>australia</td>\n      <td>53</td>\n      <td>5.35</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td></td>\n      <td>europe</td>\n      <td>42</td>\n      <td>4.24</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td></td>\n      <td>canada west</td>\n      <td>40</td>\n      <td>4.04</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td></td>\n      <td>canada east</td>\n      <td>33</td>\n      <td>3.33</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td></td>\n      <td>latin america</td>\n      <td>14</td>\n      <td>1.41</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td></td>\n      <td>asia</td>\n      <td>13</td>\n      <td>1.31</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td></td>\n      <td>africa</td>\n      <td>6</td>\n      <td>0.61</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>gender</td>\n      <td>male</td>\n      <td>813</td>\n      <td>82.04</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td></td>\n      <td>female</td>\n      <td>178</td>\n      <td>17.96</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rp.summary_cat(df[[\"region\", \"gender\"]])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The table was called a contingency table, by Karl Pearson, because the intent is to help determine whether one variable is contingent upon or depends upon the other variable."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "(25.094823394131172,\n 0.0681711614359482,\n 16,\n array([[ 1.07769929,  4.92230071],\n        [ 2.33501514, 10.66498486],\n        [ 9.51967709, 43.48032291],\n        [ 5.92734612, 27.07265388],\n        [ 7.18466196, 32.81533804],\n        [10.59737639, 48.40262361],\n        [ 7.54389506, 34.45610494],\n        [ 2.51463169, 11.48536831],\n        [13.83047427, 63.16952573],\n        [19.57820383, 89.42179617],\n        [15.98587286, 73.01412714],\n        [11.67507568, 53.32492432],\n        [11.49545913, 52.50454087],\n        [13.65085772, 62.34914228],\n        [18.68012109, 85.31987891],\n        [13.83047427, 63.16952573],\n        [12.57315843, 57.42684157]]))"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scipy.stats as stats\n",
    "\n",
    "crosstab = pd.crosstab(df[\"region\"], df[\"gender\"])\n",
    "stats.chi2_contingency(crosstab)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "                    gender               \ngender              female male    All   \nregion                                   \nafrica                0.10   0.50    0.61\nasia                  0.20   1.11    1.31\naustralia             0.61   4.74    5.35\ncanada east           0.81   2.52    3.33\ncanada west           1.21   2.83    4.04\ncentral east          1.11   4.84    5.95\neurope                0.00   4.24    4.24\nlatin america         0.00   1.41    1.41\nmid atlantic          1.51   6.26    7.77\nnorth central         2.32   8.68   11.00\nnorth east            1.41   7.57    8.98\nnorth west            1.11   5.45    6.56\nnorthern california   1.11   5.35    6.46\nsouth central         1.01   6.66    7.67\nsouth east            1.92   8.58   10.49\nsouth west            2.02   5.75    7.77\nsouthern california   1.51   5.55    7.06\nAll                  17.96  82.04  100.00",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead tr th {\n        text-align: left;\n    }\n\n    .dataframe thead tr:last-of-type th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr>\n      <th></th>\n      <th colspan=\"3\" halign=\"left\">gender</th>\n    </tr>\n    <tr>\n      <th>gender</th>\n      <th>female</th>\n      <th>male</th>\n      <th>All</th>\n    </tr>\n    <tr>\n      <th>region</th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>africa</th>\n      <td>0.10</td>\n      <td>0.50</td>\n      <td>0.61</td>\n    </tr>\n    <tr>\n      <th>asia</th>\n      <td>0.20</td>\n      <td>1.11</td>\n      <td>1.31</td>\n    </tr>\n    <tr>\n      <th>australia</th>\n      <td>0.61</td>\n      <td>4.74</td>\n      <td>5.35</td>\n    </tr>\n    <tr>\n      <th>canada east</th>\n      <td>0.81</td>\n      <td>2.52</td>\n      <td>3.33</td>\n    </tr>\n    <tr>\n      <th>canada west</th>\n      <td>1.21</td>\n      <td>2.83</td>\n      <td>4.04</td>\n    </tr>\n    <tr>\n      <th>central east</th>\n      <td>1.11</td>\n      <td>4.84</td>\n      <td>5.95</td>\n    </tr>\n    <tr>\n      <th>europe</th>\n      <td>0.00</td>\n      <td>4.24</td>\n      <td>4.24</td>\n    </tr>\n    <tr>\n      <th>latin america</th>\n      <td>0.00</td>\n      <td>1.41</td>\n      <td>1.41</td>\n    </tr>\n    <tr>\n      <th>mid atlantic</th>\n      <td>1.51</td>\n      <td>6.26</td>\n      <td>7.77</td>\n    </tr>\n    <tr>\n      <th>north central</th>\n      <td>2.32</td>\n      <td>8.68</td>\n      <td>11.00</td>\n    </tr>\n    <tr>\n      <th>north east</th>\n      <td>1.41</td>\n      <td>7.57</td>\n      <td>8.98</td>\n    </tr>\n    <tr>\n      <th>north west</th>\n      <td>1.11</td>\n      <td>5.45</td>\n      <td>6.56</td>\n    </tr>\n    <tr>\n      <th>northern california</th>\n      <td>1.11</td>\n      <td>5.35</td>\n      <td>6.46</td>\n    </tr>\n    <tr>\n      <th>south central</th>\n      <td>1.01</td>\n      <td>6.66</td>\n      <td>7.67</td>\n    </tr>\n    <tr>\n      <th>south east</th>\n      <td>1.92</td>\n      <td>8.58</td>\n      <td>10.49</td>\n    </tr>\n    <tr>\n      <th>south west</th>\n      <td>2.02</td>\n      <td>5.75</td>\n      <td>7.77</td>\n    </tr>\n    <tr>\n      <th>southern california</th>\n      <td>1.51</td>\n      <td>5.55</td>\n      <td>7.06</td>\n    </tr>\n    <tr>\n      <th>All</th>\n      <td>17.96</td>\n      <td>82.04</td>\n      <td>100.00</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crosstab, test_results, expected = rp.crosstab(df[\"region\"], df[\"gender\"],\n",
    "                                               test=\"chi-square\",\n",
    "                                               expected_freqs=True,\n",
    "                                               prop=\"cell\")\n",
    "\n",
    "crosstab"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "data": {
      "text/plain": "                 Chi-square test  results\n0  Pearson Chi-square ( 16.0) =   25.0948\n1                     p-value =    0.0682\n2                  Cramer's V =    0.1591",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Chi-square test</th>\n      <th>results</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Pearson Chi-square ( 16.0) =</td>\n      <td>25.0948</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>p-value =</td>\n      <td>0.0682</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Cramer's V =</td>\n      <td>0.1591</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_results"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### ASSUMPTION CHECK\n",
    "\n",
    "Checking the assumptions for the χ2 test of independence is easy. Let's recall what they are:\n",
    "\n",
    "+ The two samples are independent\n",
    "+ The variables were collected independently of each other, i.e. the answer from one variable was not dependent on the answer of the other\n",
    "+ No expected cell count is = 0\n",
    "+ No more than 20% of the cells have and expected cell count < 5\n",
    "The last two assumptions can be checked by looking at the expected frequency table."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "                        gender           \ngender                  female     male  \nregion                                   \nafrica                1.077699   4.922301\nasia                  2.335015  10.664985\naustralia             9.519677  43.480323\ncanada east           5.927346  27.072654\ncanada west           7.184662  32.815338\ncentral east         10.597376  48.402624\neurope                7.543895  34.456105\nlatin america         2.514632  11.485368\nmid atlantic         13.830474  63.169526\nnorth central        19.578204  89.421796\nnorth east           15.985873  73.014127\nnorth west           11.675076  53.324924\nnorthern california  11.495459  52.504541\nsouth central        13.650858  62.349142\nsouth east           18.680121  85.319879\nsouth west           13.830474  63.169526\nsouthern california  12.573158  57.426842",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead tr th {\n        text-align: left;\n    }\n\n    .dataframe thead tr:last-of-type th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr>\n      <th></th>\n      <th colspan=\"2\" halign=\"left\">gender</th>\n    </tr>\n    <tr>\n      <th>gender</th>\n      <th>female</th>\n      <th>male</th>\n    </tr>\n    <tr>\n      <th>region</th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>africa</th>\n      <td>1.077699</td>\n      <td>4.922301</td>\n    </tr>\n    <tr>\n      <th>asia</th>\n      <td>2.335015</td>\n      <td>10.664985</td>\n    </tr>\n    <tr>\n      <th>australia</th>\n      <td>9.519677</td>\n      <td>43.480323</td>\n    </tr>\n    <tr>\n      <th>canada east</th>\n      <td>5.927346</td>\n      <td>27.072654</td>\n    </tr>\n    <tr>\n      <th>canada west</th>\n      <td>7.184662</td>\n      <td>32.815338</td>\n    </tr>\n    <tr>\n      <th>central east</th>\n      <td>10.597376</td>\n      <td>48.402624</td>\n    </tr>\n    <tr>\n      <th>europe</th>\n      <td>7.543895</td>\n      <td>34.456105</td>\n    </tr>\n    <tr>\n      <th>latin america</th>\n      <td>2.514632</td>\n      <td>11.485368</td>\n    </tr>\n    <tr>\n      <th>mid atlantic</th>\n      <td>13.830474</td>\n      <td>63.169526</td>\n    </tr>\n    <tr>\n      <th>north central</th>\n      <td>19.578204</td>\n      <td>89.421796</td>\n    </tr>\n    <tr>\n      <th>north east</th>\n      <td>15.985873</td>\n      <td>73.014127</td>\n    </tr>\n    <tr>\n      <th>north west</th>\n      <td>11.675076</td>\n      <td>53.324924</td>\n    </tr>\n    <tr>\n      <th>northern california</th>\n      <td>11.495459</td>\n      <td>52.504541</td>\n    </tr>\n    <tr>\n      <th>south central</th>\n      <td>13.650858</td>\n      <td>62.349142</td>\n    </tr>\n    <tr>\n      <th>south east</th>\n      <td>18.680121</td>\n      <td>85.319879</td>\n    </tr>\n    <tr>\n      <th>south west</th>\n      <td>13.830474</td>\n      <td>63.169526</td>\n    </tr>\n    <tr>\n      <th>southern california</th>\n      <td>12.573158</td>\n      <td>57.426842</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expected"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [
    {
     "data": {
      "text/plain": "[[95, 55], [103, 247]]"
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table = [[95, 55],\n",
    "         [103, 247]]\n",
    "\n",
    "table"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[95, 55], [103, 247]]\n",
      "\n",
      "\n",
      "\n",
      "dof=1\n",
      "\n",
      "\n",
      "\n",
      "[[ 59.4  90.6]\n",
      " [138.6 211.4]]\n",
      "\n",
      "\n",
      "\n",
      "probability=0.990, critical=6.635, stat=49.056\n",
      "\n",
      "\n",
      "\n",
      "Dependent (reject H0)\n",
      "\n",
      "\n",
      "\n",
      "significance=0.010, p=0.000\n",
      "Dependent (reject H0)\n"
     ]
    }
   ],
   "source": [
    "# chi-squared test with similar proportions\n",
    "from scipy.stats import chi2_contingency\n",
    "from scipy.stats import chi2\n",
    "\n",
    "# contingency table\n",
    "table = table\n",
    "print(table)\n",
    "stat, p, dof, expected = chi2_contingency(table)\n",
    "print('\\n\\n')\n",
    "\n",
    "print('dof=%d' % dof)\n",
    "print('\\n\\n')\n",
    "\n",
    "print(expected)\n",
    "# interpret test-statistic\n",
    "prob = 0.99\n",
    "critical = chi2.ppf(prob, dof)\n",
    "print('\\n\\n')\n",
    "print('probability=%.3f, critical=%.3f, stat=%.3f' % (prob, critical, stat))\n",
    "print('\\n\\n')\n",
    "if abs(stat) >= critical:\n",
    "    print('Dependent (reject H0)')\n",
    "else:\n",
    "    print('Independent (fail to reject H0)')\n",
    "print('\\n\\n')\n",
    "# interpret p-value\n",
    "alpha = 1.0 - prob\n",
    "print('significance=%.3f, p=%.3f' % (alpha, p))\n",
    "if p <= alpha:\n",
    "    print('Dependent (reject H0)')\n",
    "else:\n",
    "    print('Independent (fail to reject H0)')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Summary\n",
    "This section lists some ideas for extending the tutorial that you may wish to explore.\n",
    "\n",
    "Update the chi-squared test to use your own contingency table.\n",
    "Write a function to report on the independence given observations from two categorical variables\n",
    "Load a standard machine learning dataset containing categorical variables and report on the independence of each.\n",
    "\n",
    "Pairs of categorical variables can be summarized using a contingency table.\n",
    "The chi-squared test can compare an observed contingency table to an expected table and determine if the categorical variables are independent.\n",
    "How to calculate and interpret the chi-squared test for categorical variables in Python.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## --------> OBSERVATION\n",
    "The one piece of information that researchpy calculates that scipy.stats does not is a measure of the strength of the relationship - this is akin to a correlation statistic such as Pearson's correlation coefficient. A good peer-reviewed article that is not behind a paywall is written by Akoglu (2018). The following table is reproduced from the mentioned article."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook EDA.ipynb to python\n",
      "[NbConvertApp] Writing 16116 bytes to EDA.py\n",
      "[NbConvertApp] Converting notebook preprocessing.ipynb to python\n",
      "[NbConvertApp] Writing 9911 bytes to preprocessing.py\n",
      "[NbConvertApp] Converting notebook quiz_3.ipynb to python\n",
      "[NbConvertApp] Writing 12673 bytes to quiz_3.py\n",
      "[NbConvertApp] Converting notebook normality_test.ipynb to python\n",
      "[NbConvertApp] Writing 9797 bytes to normality_test.py\n",
      "[NbConvertApp] Converting notebook quiz_2.ipynb to python\n",
      "[NbConvertApp] ERROR | Notebook JSON is invalid: data.cells[{data__cells_x}] must be valid exactly by one definition (0 matches found)\n",
      "\n",
      "Failed validating <unset> in notebook['data']['cells']:\n",
      "\n",
      "On instance:\n",
      "<unset>\n",
      "[NbConvertApp] Writing 9284 bytes to quiz_2.py\n",
      "[NbConvertApp] Converting notebook statistical_methods.ipynb to python\n",
      "[NbConvertApp] Writing 3015 bytes to statistical_methods.py\n",
      "[NbConvertApp] Converting notebook demo_normality_test.ipynb to python\n",
      "[NbConvertApp] Writing 3950 bytes to demo_normality_test.py\n"
     ]
    }
   ],
   "source": [
    "# The .py format of the jupyter notebook\n",
    "import os\n",
    "\n",
    "for fname in os.listdir():\n",
    "    if fname.endswith('ipynb'):\n",
    "        os.system(f'jupyter nbconvert {fname} --to python')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "# References\n",
    "Seabold, Skipper, and Josef Perktold. “statsmodels: Econometric and statistical modeling with python.” Proceedings of the 9th Python in Science Conference. 2010.\n",
    "Virtanen P, Gommers R, Oliphant TE, Haberland M, Reddy T, Cournapeau D, Burovski E, Peterson P, Weckesser W, Bright J, van der Walt SJ. SciPy 1.0: fundamental algorithms for scientific computing in Python. Nature methods. 2020 Mar;17(3):261-72.\n",
    "Mangiafico, S.S. 2015. An R Companion for the Handbook of Biological Statistics, version 1.3.2.\n",
    "Knief U, Forstmeier W. Violating the normality assumption may be the lesser of two evils. bioRxiv. 2018 Jan 1:498931.\n",
    "Kozak M, Piepho HP. What’s normal anyway? Residual plots are more telling than significance tests when checking ANOVA assumptions. Journal of Agronomy and Crop Science. 2018 Feb;204(1):86-98."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
