{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ One way Analysis of Variance (ANOVA)\n",
    "+ Kruskal Wallis test\n",
    "+ Chi Square Goodness of Fit test\n",
    "+ Chi Square Tests for Independence / Association Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: researchpy in /Users/tnathu-ai/opt/anaconda3/lib/python3.9/site-packages (0.3.5)\n",
      "Requirement already satisfied: statsmodels in /Users/tnathu-ai/opt/anaconda3/lib/python3.9/site-packages (from researchpy) (0.13.2)\n",
      "Requirement already satisfied: numpy in /Users/tnathu-ai/opt/anaconda3/lib/python3.9/site-packages (from researchpy) (1.21.5)\n",
      "Requirement already satisfied: pandas in /Users/tnathu-ai/opt/anaconda3/lib/python3.9/site-packages (from researchpy) (1.4.2)\n",
      "Requirement already satisfied: scipy in /Users/tnathu-ai/opt/anaconda3/lib/python3.9/site-packages (from researchpy) (1.7.3)\n",
      "Requirement already satisfied: patsy in /Users/tnathu-ai/opt/anaconda3/lib/python3.9/site-packages (from researchpy) (0.5.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /Users/tnathu-ai/opt/anaconda3/lib/python3.9/site-packages (from pandas->researchpy) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/tnathu-ai/opt/anaconda3/lib/python3.9/site-packages (from pandas->researchpy) (2021.3)\n",
      "Requirement already satisfied: six>=1.5 in /Users/tnathu-ai/opt/anaconda3/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas->researchpy) (1.16.0)\n",
      "Requirement already satisfied: packaging>=21.3 in /Users/tnathu-ai/opt/anaconda3/lib/python3.9/site-packages (from statsmodels->researchpy) (21.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/tnathu-ai/opt/anaconda3/lib/python3.9/site-packages (from packaging>=21.3->statsmodels->researchpy) (3.0.4)\n"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "!pip install researchpy\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import researchpy as rp\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numeric columns:  ['athlete_id', 'age', 'height', 'weight', 'fran', 'helen', 'grace', 'filthy50', 'fgonebad', 'run400', 'run5k', 'candj', 'snatch', 'deadlift', 'backsq', 'pullups', 'rank', 'score'] \n",
      "\n",
      "String columns:  ['name', 'region', 'team', 'affiliate', 'gender', 'eat', 'train', 'background', 'experience', 'schedule', 'division'] \n",
      "\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 991 entries, 0 to 990\n",
      "Data columns (total 29 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   athlete_id  991 non-null    Int64 \n",
      " 1   name        991 non-null    string\n",
      " 2   region      991 non-null    string\n",
      " 3   team        991 non-null    string\n",
      " 4   affiliate   991 non-null    string\n",
      " 5   gender      991 non-null    string\n",
      " 6   age         991 non-null    Int64 \n",
      " 7   height      991 non-null    Int64 \n",
      " 8   weight      991 non-null    Int64 \n",
      " 9   fran        991 non-null    Int64 \n",
      " 10  helen       991 non-null    Int64 \n",
      " 11  grace       991 non-null    Int64 \n",
      " 12  filthy50    991 non-null    Int64 \n",
      " 13  fgonebad    991 non-null    Int64 \n",
      " 14  run400      991 non-null    Int64 \n",
      " 15  run5k       991 non-null    Int64 \n",
      " 16  candj       991 non-null    Int64 \n",
      " 17  snatch      991 non-null    Int64 \n",
      " 18  deadlift    991 non-null    Int64 \n",
      " 19  backsq      991 non-null    Int64 \n",
      " 20  pullups     991 non-null    Int64 \n",
      " 21  eat         991 non-null    string\n",
      " 22  train       991 non-null    string\n",
      " 23  background  991 non-null    string\n",
      " 24  experience  991 non-null    string\n",
      " 25  schedule    991 non-null    string\n",
      " 26  division    991 non-null    string\n",
      " 27  rank        991 non-null    Int64 \n",
      " 28  score       991 non-null    Int64 \n",
      "dtypes: Int64(18), string(11)\n",
      "memory usage: 242.1 KB\n",
      "The shape and df type of the ORGINAL df: None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>athlete_id</th>\n",
       "      <th>name</th>\n",
       "      <th>region</th>\n",
       "      <th>team</th>\n",
       "      <th>affiliate</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>height</th>\n",
       "      <th>weight</th>\n",
       "      <th>fran</th>\n",
       "      <th>...</th>\n",
       "      <th>backsq</th>\n",
       "      <th>pullups</th>\n",
       "      <th>eat</th>\n",
       "      <th>train</th>\n",
       "      <th>background</th>\n",
       "      <th>experience</th>\n",
       "      <th>schedule</th>\n",
       "      <th>division</th>\n",
       "      <th>rank</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2720</td>\n",
       "      <td>justin adams</td>\n",
       "      <td>south central</td>\n",
       "      <td>woodward crossfit</td>\n",
       "      <td>woodward crossfit</td>\n",
       "      <td>male</td>\n",
       "      <td>24</td>\n",
       "      <td>68</td>\n",
       "      <td>180</td>\n",
       "      <td>126</td>\n",
       "      <td>...</td>\n",
       "      <td>405</td>\n",
       "      <td>80</td>\n",
       "      <td>i eat quality foods but don't measure the amount|</td>\n",
       "      <td>i workout mostly at a crossfit affiliate|i hav...</td>\n",
       "      <td>i played youth or high school level sports|</td>\n",
       "      <td>i began crossfit with a coach (e.g. at an affi...</td>\n",
       "      <td>i do multiple workouts in a day 3+ times a wee...</td>\n",
       "      <td>male</td>\n",
       "      <td>3448</td>\n",
       "      <td>464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6922</td>\n",
       "      <td>daniel adamson</td>\n",
       "      <td>south west</td>\n",
       "      <td>crossfit the point</td>\n",
       "      <td>crossfit the point</td>\n",
       "      <td>male</td>\n",
       "      <td>31</td>\n",
       "      <td>67</td>\n",
       "      <td>150</td>\n",
       "      <td>244</td>\n",
       "      <td>...</td>\n",
       "      <td>330</td>\n",
       "      <td>42</td>\n",
       "      <td>i eat quality foods but don't measure the amount|</td>\n",
       "      <td>i workout mostly at a crossfit affiliate|i inc...</td>\n",
       "      <td>i played college sports|</td>\n",
       "      <td>i began crossfit by trying it alone (without a...</td>\n",
       "      <td>i usually only do 1 workout a day|i do multipl...</td>\n",
       "      <td>male</td>\n",
       "      <td>35748</td>\n",
       "      <td>712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12563</td>\n",
       "      <td>steven lee adams</td>\n",
       "      <td>mid atlantic</td>\n",
       "      <td>crossfit kaiju</td>\n",
       "      <td>crossfit kaiju</td>\n",
       "      <td>male</td>\n",
       "      <td>37</td>\n",
       "      <td>72</td>\n",
       "      <td>210</td>\n",
       "      <td>162</td>\n",
       "      <td>...</td>\n",
       "      <td>425</td>\n",
       "      <td>49</td>\n",
       "      <td>i eat quality foods but don't measure the amou...</td>\n",
       "      <td>i workout mostly at a crossfit affiliate|i inc...</td>\n",
       "      <td>i played youth or high school level sports|i p...</td>\n",
       "      <td>i began crossfit with a coach (e.g. at an affi...</td>\n",
       "      <td>i do multiple workouts in a day 2x a week|i ty...</td>\n",
       "      <td>male</td>\n",
       "      <td>5073</td>\n",
       "      <td>485</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   athlete_id              name         region                team  \\\n",
       "0        2720      justin adams  south central   woodward crossfit   \n",
       "1        6922    daniel adamson     south west  crossfit the point   \n",
       "2       12563  steven lee adams   mid atlantic      crossfit kaiju   \n",
       "\n",
       "            affiliate gender  age  height  weight  fran  ...  backsq  pullups  \\\n",
       "0   woodward crossfit   male   24      68     180   126  ...     405       80   \n",
       "1  crossfit the point   male   31      67     150   244  ...     330       42   \n",
       "2      crossfit kaiju   male   37      72     210   162  ...     425       49   \n",
       "\n",
       "                                                 eat  \\\n",
       "0  i eat quality foods but don't measure the amount|   \n",
       "1  i eat quality foods but don't measure the amount|   \n",
       "2  i eat quality foods but don't measure the amou...   \n",
       "\n",
       "                                               train  \\\n",
       "0  i workout mostly at a crossfit affiliate|i hav...   \n",
       "1  i workout mostly at a crossfit affiliate|i inc...   \n",
       "2  i workout mostly at a crossfit affiliate|i inc...   \n",
       "\n",
       "                                          background  \\\n",
       "0        i played youth or high school level sports|   \n",
       "1                           i played college sports|   \n",
       "2  i played youth or high school level sports|i p...   \n",
       "\n",
       "                                          experience  \\\n",
       "0  i began crossfit with a coach (e.g. at an affi...   \n",
       "1  i began crossfit by trying it alone (without a...   \n",
       "2  i began crossfit with a coach (e.g. at an affi...   \n",
       "\n",
       "                                            schedule  division   rank  score  \n",
       "0  i do multiple workouts in a day 3+ times a wee...      male   3448    464  \n",
       "1  i usually only do 1 workout a day|i do multipl...      male  35748    712  \n",
       "2  i do multiple workouts in a day 2x a week|i ty...      male   5073    485  \n",
       "\n",
       "[3 rows x 29 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set the general path of the external df\n",
    "external_df_path = os.path.join(os.path.pardir, 'data', 'interim')\n",
    "\n",
    "# set the path for specific dfset from external dfset\n",
    "df = os.path.join(external_df_path, 'cleaned_data.csv')\n",
    "\n",
    "# import dfset\n",
    "df = pd.read_csv(df, delimiter=',', skipinitialspace=True)\n",
    "\n",
    "# convert columns to the best possible dtypes, object->string\n",
    "df = df.convert_dtypes()\n",
    "\n",
    "# select numeric columns\n",
    "df_numeric = df.select_dtypes(include=[np.number]).columns.to_list()\n",
    "\n",
    "# select non-numeric columns\n",
    "df_string = df.select_dtypes(include='string').columns.tolist()\n",
    "\n",
    "print(\"Numeric columns: \", df_numeric, \"\\n\")\n",
    "print(\"String columns: \", df_string, \"\\n\\n\")\n",
    "\n",
    "# print dfset info\n",
    "print(\"The shape and df type of the ORGINAL df:\", str(df.info()))\n",
    "\n",
    "# print first 3 rows\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HOMOGENEITY OF VARIANCE\n",
    "\n",
    "testing this assumption is the Levene's test of homogeneity of variances. This can be completed using the levene() method from Scipy.stats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# stats.levene(*[df['score'][df['region'] == region] for region in df.region.unique()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## -------> OBSERVATION\n",
    "The Levene's test of homogeneity of variances is not significant which indicates that the groups have non-statistically significant difference in their varability. Again, it may be worthwhile to check this assumption visually as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ------> OBSERVATIONS\n",
    "The graphical testing of homogeneity of variances supports the statistical testing findings which is the groups have equal variance.\n",
    "\n",
    "By default box plots show the median (orange line in graph above). The green triangle is the mean for each group which was an additional argument that was passed into the method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:#ffc0cb;font-size:40px;font-family:Georgia;text-align:center;\"><strong>One way Analysis of Variance (ANOVA)</strong></h1>\n",
    "\n",
    "## ANOVA Hypotheses\n",
    "+ Null hypothesis: Groups means are equal (no variation in means of groups)\n",
    "H0: μ1=μ2=…=μp\n",
    "+ Alternative hypothesis: At least, one group mean is different from other groups\n",
    "H1: All μ are not equal\n",
    "\n",
    "### Parametric test assumptions\n",
    "+ Population distributions are normal\n",
    "+ Samples have equal variances\n",
    "+ Independence\n",
    "\n",
    "## ANOVA Assumptions\n",
    "+ Residuals (experimental error) are approximately normally distributed (Shapiro-Wilks test or histogram)homoscedasticity or Homogeneity of variances (variances are equal between treatment groups) (Levene’s, Bartlett’s, or Brown-Forsythe test)\n",
    "+ Observations are sampled independently from each other (no relation in observations between the groups and within the groups) i.e., each subject should have only one response\n",
    "+ The dependent variable should be continuous. If the dependent variable is ordinal or rank (e.g. Likert item data), it is more likely to violate the assumptions of normality and homogeneity of variances. If these assumptions are violated, you should consider the non-parametric tests (e.g. Mann-Whitney U test, Kruskal-Wallis test).\n",
    "\n",
    "\n",
    "## How ANOVA works?\n",
    "+ Check sample sizes: equal number of observation in each group\n",
    "+ Calculate Mean Square for each group (MS) (SS of group/level-1); level-1 is a degrees of freedom (df) for a group\n",
    "+ Calculate Mean Square error (MSE) (SS error/df of residuals)\n",
    "+ Calculate F value (MS of group/MSE)\n",
    "+ Calculate p value based on F value and degrees of freedom (df)\n",
    "\n",
    "## Questions?\n",
    "+ Imbalance label problem (unequal sample size for each group) data\n",
    "\n",
    "# ANOVA TABLE\n",
    "\n",
    "| **ANOVA Source** |   **df**   | **SS** | **MS**          | **F**    | **Notes**           |\n",
    "|:----------------:|:----------:|:------:|:---------------:|:--------:|:-------------------:|\n",
    "|  **Treatments**  | dfTr = k-1 |  SSTr  | MSTr=SSTr/(k-1) | MSTr/MSE | k: number of groups |\n",
    "|    **Errors**    | dfE = n-k  | SSE    | MSE=SSE/(n-k)   |          | n: sample size      |\n",
    "|    **Total**     | dfT = n-1  | SST    |                 |          |                     |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "k = 0\n",
    "n = 0\n",
    "dfTr = 0\n",
    "dfE = 0\n",
    "dfT = 0\n",
    "SSTr = 0\n",
    "SSE = 0\n",
    "SST = 0\n",
    "MSTr = 0\n",
    "MSE = 0\n",
    "F = 0\n",
    "p = 0\n",
    "\n",
    "dfTr = k - 1\n",
    "dfE = n - k\n",
    "dfT = n - 1\n",
    "SSTr = SST - SSE\n",
    "MSTr = SSTr / dfTr\n",
    "MSE = SSE / dfE\n",
    "SST = SSTr + SSE\n",
    "F = MSTr / MSE\n",
    "p = stats.f.sf(F, dfTr, dfE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F value:  p value:  1.4139522059029603e-05\n"
     ]
    }
   ],
   "source": [
    "n = 18\n",
    "k = 3\n",
    "dfT = 17\n",
    "SSTr = 57.11\n",
    "SST = 73.75\n",
    "\n",
    "dfTr = k - 1\n",
    "dfE = n - k\n",
    "MSTr = SSTr / dfTr\n",
    "SSE = SST - SSTr\n",
    "MSE = SSE / dfE\n",
    "F = MSTr / MSE\n",
    "p = stats.f.sf(F, dfTr, dfE)\n",
    "print(\"F value: \", F and \"p value: \", p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# F Critical Value\n",
    "\n",
    "`scipy.stats.f.ppf(q, dfn, dfd)`\n",
    "\n",
    "where:\n",
    "\n",
    "`q`: The significance level to use\n",
    "`dfn`: The numerator degrees of freedom\n",
    "`dfd`: The denominator degrees of freedom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "Steps in the test:\n",
    "1. First write down the null and alternate hypothesis.\n",
    "    >> H0 : μ1 =μ2 =μ3 =···=μk\n",
    "    >> H1 : at least one pair have different means.\n",
    "2. Next calculate the test statistic F using the data and present it in a\n",
    "table as above.\n",
    "3. Find the Rejection region for the corresponding alternate hypothesis\n",
    "and chosen α value, that is find Fα,k−1,n−k. **The rejection area is in the right hand side of the critical value.\n",
    "4. Reject or Don’t Reject If the test statistic F falls in the rejection region, reject H0 and conclude H1 is true, or else do not reject H0. You should also interpret the result in words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numerator df v1: 12\n",
      "Denominator df v2: 6\n",
      "The critical value from F-table: F(0.95, 2, 15) = 3.6823203436732412\n",
      "F critical value: 3.9999353833188764\n"
     ]
    }
   ],
   "source": [
    "import scipy.stats\n",
    "\n",
    "# F0.05,2,12 = 3.89\n",
    "# sample size\n",
    "n = 18\n",
    "# number of groups\n",
    "k = 3\n",
    "# significance level\n",
    "q = 1 - .05\n",
    "\n",
    "# numerator degrees of freedom\n",
    "dfn = 12\n",
    "print(f'Numerator df v1: {dfn}')\n",
    "\n",
    "# denominator degrees of freedom\n",
    "dfd = 6\n",
    "print(f'Denominator df v2: {dfd}')\n",
    "\n",
    "# F critical value\n",
    "print(f'The critical value from F-table: F({q}, {k-1}, {n-k}) = {scipy.stats.f.ppf(q, k-1, n-k)}')\n",
    "\n",
    "#find F critical value\n",
    "print(f'F critical value: {scipy.stats.f.ppf(q=q, dfn=dfn, dfd=dfd)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The critical value from F-table: F(0.95, 2, 15) = 3.6823203436732412\n",
      "Numerator df v1: 2\n",
      "Denominator df v2: 15\n",
      "F critical value: 3.6823203436732412\n"
     ]
    }
   ],
   "source": [
    "import scipy.stats\n",
    "\n",
    "# F0.05,2,12 = 3.89\n",
    "# sample size\n",
    "n = 18\n",
    "# number of groups\n",
    "k = 3\n",
    "# significance level\n",
    "q = 1 - .05\n",
    "\n",
    "# numerator degrees of freedom\n",
    "dfn = k - 1\n",
    "print(f'Numerator df v1: {dfn}')\n",
    "\n",
    "# denominator degrees of freedom\n",
    "dfd = n - k\n",
    "print(f'Denominator df v2: {dfd}')\n",
    "\n",
    "# F critical value\n",
    "print(f'The critical value from F-table: F({q}, {k-1}, {n-k}) = {scipy.stats.f.ppf(q, k-1, n-k)}')\n",
    "\n",
    "#find F critical value\n",
    "print(f'F critical value: {scipy.stats.f.ppf(q=q, dfn=dfn, dfd=dfd)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F critical value: 3.492828476735632\n"
     ]
    }
   ],
   "source": [
    "import scipy.stats\n",
    "\n",
    "# F0.05,2,12 = 3.89\n",
    "\n",
    "performance1 = [8, 9, 11, 10, 11, 9, 10, 10]\n",
    "performance2 = [13, 10, 9, 10, 11, 8, 14, 13]\n",
    "performance3 = [10, 12, 13, 10, 11, 11, 13]\n",
    "\n",
    "# number of values in all groups\n",
    "n = len(performance1) + len(performance2) + len(performance3)\n",
    "# number of groups\n",
    "k = 3\n",
    "# significance level\n",
    "q = 1 - .05\n",
    "# numerator degrees of freedom\n",
    "dfn = k - 1\n",
    "# denominator degrees of freedom\n",
    "dfd = n - k\n",
    "#find F critical value\n",
    "print(f'F critical value: {scipy.stats.f.ppf(q=q, dfn=dfn, dfd=dfd)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "F_onewayResult(statistic=2.368271597147726, pvalue=0.11937403275881779)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing library\n",
    "from scipy.stats import f_oneway\n",
    "\n",
    "performance1 = [8, 9, 11, 10, 11, 9, 10, 10]\n",
    "performance2 = [13, 10, 9, 10, 11, 8, 14, 13]\n",
    "performance3 = [10, 12, 13, 10, 11, 11, 13]\n",
    "\n",
    "# Conduct the one-way ANOVA\n",
    "f_oneway(performance1, performance2, performance3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### -------> OBSERVATION\n",
    "\n",
    "The purpose of this study was to test for a difference in score between the region. The overall average score was 610.55 95% CI(600.81,620.29) with group averages of.... There is a statistically insignificant difference between the regions and their effects the scores, F= 1.19, p-value= 0.27.\n",
    "\n",
    "As the p value (0.27) is insignificant, we fail to reject the null hypothesis and conclude that regions have equal variances.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:#ffc0cb;font-size:40px;font-family:Georgia;text-align:center;\"><strong>Kruskal Wallis test</strong></h1>\n",
    "\n",
    "\n",
    "Assumptions for the test:\n",
    "1. At least one of the two large sample conditions are met.\n",
    "2. All the samples are random samples.\n",
    "3. All the populations being sampled have the same shaped probability density function, with possibly different means.\n",
    "4. The populations are independent.\n",
    "\n",
    "H0 : μ1 =μ2 =μ3 =···=μk\n",
    "H1 : at least two μi differ.\n",
    "\n",
    "**NOTE**: The larger the differences the larger the test statistic H. This is why the test is only an upper tail test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "degrees of freedom: 3\n",
      "The critical value X²U for the upper tail is 11.344866730144373\n"
     ]
    }
   ],
   "source": [
    "# Find the Chi-Square Critical Value\n",
    "import scipy.stats\n",
    "\n",
    "# find Chi-Square critical value for 2 tail hypothesis tests\n",
    "alpha = float(0.01)\n",
    "k = 4\n",
    "degree_freedom = k - 1\n",
    "print(f'degrees of freedom: {(k - 1)}')\n",
    "# X² for upper tail\n",
    "print(f'The critical value X²U for the upper tail is {scipy.stats.chi2.ppf(1 - alpha, df=degree_freedom)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KruskalResult(statistic=21.644331786306708, pvalue=0.15505130496906336)\n"
     ]
    }
   ],
   "source": [
    "# Conduct the Kruskal-Wallis Test\n",
    "result = stats.kruskal(*[df['score'][df['region'] == region] for region in df.region.unique()])\n",
    "\n",
    "# Print the result\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KruskalResult(statistic=2.0390527509926217, pvalue=0.5643408523150142)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# group1 = [624, 680, 454, 510, 539]\n",
    "# group2 = [425, 595, 737, 459, 709, 482]\n",
    "# group3 = [397, 794, 595, 539, 680, 652]\n",
    "# group4 = [482, 510, 369, 567, 595]\n",
    "#\n",
    "#\n",
    "# from scipy import stats\n",
    "#\n",
    "# #perform Kruskal-Wallis Test\n",
    "# stats.kruskal(group1, group2, group3, group4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:#ffc0cb;font-size:40px;font-family:Georgia;text-align:center;\"><strong>Chi Square Goodness of Fit test</strong></h1>\n",
    "\n",
    "The Chi-Square Goodness of fit test is a non-parametric statistical hypothesis test that’s used to determine how considerably the observed value of an event differs from the expected value. it helps us check whether a variable comes from a certain distribution or if a sample represents a population. The observed probability distribution is compared with the expected probability distribution.\n",
    "\n",
    "\n",
    "if chi_square_ value > critical value, the null hypothesis is rejected. if chi_square_ value <= critical value, the null hypothesis is accepted.\n",
    "\n",
    "\n",
    "H0: (null hypothesis) A variable follows a hypothesized distribution.\n",
    "H1: (alternative hypothesis) A variable does not follow a hypothesized distribution.\n",
    "\n",
    "`chisquare(f_obs, f_exp)`\n",
    "\n",
    "where:\n",
    "\n",
    "`f_obs`: An array of observed counts.\n",
    "`f_exp`: An array of expected counts. By default, each category is assumed to be equally likely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Power_divergenceResult(statistic=23.96825396825397, pvalue=2.5364108090024436e-05)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First, we will create two arrays to hold our observed and expected number of customers for each day:\n",
    "observed = [40, 40, 20, 20]\n",
    "expected = [32, 36, 10, 42]\n",
    "\n",
    "import scipy.stats as stats\n",
    "\n",
    "#perform Chi-Square Goodness of Fit Test\n",
    "stats.chisquare(f_obs=observed, f_exp=expected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the p-value corresponds to a Chi-Square value with n-1 degrees of freedom (dof), where n is the number of different categories. In this case, dof = 5-1 = 4. You can use the Chi-Square to P Value Calculator to confirm that the p-value that corresponds to X2 = 4.36 with dof = 4 is 0.35947.\n",
    "\n",
    "Since the p-value (.35947) is not less than 0.05, we fail to reject the null hypothesis. This means we do not have sufficient evidence to say that the true distribution of customers is different from the distribution that the shop owner claimed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A genetics experiment, crossing two types of sorghum, in theory, should produce offspring with the colours red, yellow, and white in the ratio 9:3:4.\n",
    "The outcome for 368 experimental plants was 195 red, 73 yellow, and 100 white. Does this data contradict the theory, using α = 0.01?\n",
    "The probabilities are: red, 9/16; yellow, 3/16, and white, 4/16. If we let red be outcome 1, yellow be outcome 2, and white outcome 3, we have the following null and alternate hypothesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Degree of freedom is: 3\n",
      "chi_square_test_statistic is : 23.96825396825397\n",
      "p_value : 2.5364108090024436e-05\n",
      "7.814727903251179\n"
     ]
    }
   ],
   "source": [
    "# importing packages\n",
    "import scipy.stats as stats\n",
    "\n",
    "# n = 168\n",
    "# p1=0.35\n",
    "# p2=0.35\n",
    "# p3=0.2\n",
    "# p4=0.1\n",
    "# no of hours a student studies\n",
    "# in a week vs expected no of hours\n",
    "observed = [40, 40, 20, 20]\n",
    "# expected = [p1*n, p2*n, p3*n, p4*n]\n",
    "expected = [32, 36, 10, 42]\n",
    "\n",
    "degree_freedom = int(len(observed) - 1)\n",
    "alpha = float(0.05)\n",
    "\n",
    "# Chi-Square Goodness of Fit Test\n",
    "chi_square_test_statistic, p_value = stats.chisquare(\n",
    "    observed, expected)\n",
    "\n",
    "print(f'Degree of freedom is: {degree_freedom}')\n",
    "# chi square test statistic and p value\n",
    "print('chi_square_test_statistic is : ' +\n",
    "      str(chi_square_test_statistic))\n",
    "print('p_value : ' + str(p_value))\n",
    "\n",
    "# find Chi-Square critical value\n",
    "print(stats.chi2.ppf(1 - alpha, df=degree_freedom))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:#ffc0cb;font-size:40px;font-family:Georgia;text-align:center;\"><strong>Chi Square Test for Independence</strong></h1>\n",
    "\n",
    "\n",
    "### χ^2 test of independence assumptions\n",
    "+ The two samples are independent\n",
    "+ No expected cell count is = 0\n",
    "+ No more than 20% of the cells have and expected cell count < 5\n",
    "\n",
    "![](../media/images/tests_for_Independence.png)\n",
    "\n",
    "The null and alternate hypothesis in this problem are,\n",
    "H0 : The two factors are independent.\n",
    "H1 : the two factors are dependent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "degrees of freedom: 4\n",
      "The critical value X²U for the upper tail is 9.487729036781154\n"
     ]
    }
   ],
   "source": [
    "# Find the Chi-Square Critical Value\n",
    "import scipy.stats\n",
    "\n",
    "# find Chi-Square critical value for 2 tail hypothesis tests\n",
    "alpha = float(0.05)\n",
    "rows = 2\n",
    "cols = 5\n",
    "degree_freedom = (rows - 1) * (cols - 1)\n",
    "print(f'degrees of freedom: {(rows - 1) * (cols - 1)}')\n",
    "# X² for upper tail\n",
    "print(f'The critical value X²U for the upper tail is {scipy.stats.chi2.ppf(1 - alpha, df=degree_freedom)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Variable</th>\n",
       "      <th>Outcome</th>\n",
       "      <th>Count</th>\n",
       "      <th>Percent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>region</td>\n",
       "      <td>north central</td>\n",
       "      <td>109</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td></td>\n",
       "      <td>south east</td>\n",
       "      <td>104</td>\n",
       "      <td>10.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td></td>\n",
       "      <td>north east</td>\n",
       "      <td>89</td>\n",
       "      <td>8.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td></td>\n",
       "      <td>south west</td>\n",
       "      <td>77</td>\n",
       "      <td>7.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td></td>\n",
       "      <td>mid atlantic</td>\n",
       "      <td>77</td>\n",
       "      <td>7.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td></td>\n",
       "      <td>south central</td>\n",
       "      <td>76</td>\n",
       "      <td>7.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td></td>\n",
       "      <td>southern california</td>\n",
       "      <td>70</td>\n",
       "      <td>7.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td></td>\n",
       "      <td>north west</td>\n",
       "      <td>65</td>\n",
       "      <td>6.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td></td>\n",
       "      <td>northern california</td>\n",
       "      <td>64</td>\n",
       "      <td>6.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td></td>\n",
       "      <td>central east</td>\n",
       "      <td>59</td>\n",
       "      <td>5.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td></td>\n",
       "      <td>australia</td>\n",
       "      <td>53</td>\n",
       "      <td>5.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td></td>\n",
       "      <td>europe</td>\n",
       "      <td>42</td>\n",
       "      <td>4.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td></td>\n",
       "      <td>canada west</td>\n",
       "      <td>40</td>\n",
       "      <td>4.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td></td>\n",
       "      <td>canada east</td>\n",
       "      <td>33</td>\n",
       "      <td>3.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td></td>\n",
       "      <td>latin america</td>\n",
       "      <td>14</td>\n",
       "      <td>1.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td></td>\n",
       "      <td>asia</td>\n",
       "      <td>13</td>\n",
       "      <td>1.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td></td>\n",
       "      <td>africa</td>\n",
       "      <td>6</td>\n",
       "      <td>0.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>gender</td>\n",
       "      <td>male</td>\n",
       "      <td>813</td>\n",
       "      <td>82.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td></td>\n",
       "      <td>female</td>\n",
       "      <td>178</td>\n",
       "      <td>17.96</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Variable              Outcome  Count  Percent\n",
       "0    region        north central    109     11.0\n",
       "1                     south east    104    10.49\n",
       "2                     north east     89     8.98\n",
       "3                     south west     77     7.77\n",
       "4                   mid atlantic     77     7.77\n",
       "5                  south central     76     7.67\n",
       "6            southern california     70     7.06\n",
       "7                     north west     65     6.56\n",
       "8            northern california     64     6.46\n",
       "9                   central east     59     5.95\n",
       "10                     australia     53     5.35\n",
       "11                        europe     42     4.24\n",
       "12                   canada west     40     4.04\n",
       "13                   canada east     33     3.33\n",
       "14                 latin america     14     1.41\n",
       "15                          asia     13     1.31\n",
       "16                        africa      6     0.61\n",
       "17   gender                 male    813    82.04\n",
       "18                        female    178    17.96"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rp.summary_cat(df[[\"region\", \"gender\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The table was called a contingency table, by Karl Pearson, because the intent is to help determine whether one variable is contingent upon or depends upon the other variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25.094823394131172,\n",
       " 0.0681711614359482,\n",
       " 16,\n",
       " array([[ 1.07769929,  4.92230071],\n",
       "        [ 2.33501514, 10.66498486],\n",
       "        [ 9.51967709, 43.48032291],\n",
       "        [ 5.92734612, 27.07265388],\n",
       "        [ 7.18466196, 32.81533804],\n",
       "        [10.59737639, 48.40262361],\n",
       "        [ 7.54389506, 34.45610494],\n",
       "        [ 2.51463169, 11.48536831],\n",
       "        [13.83047427, 63.16952573],\n",
       "        [19.57820383, 89.42179617],\n",
       "        [15.98587286, 73.01412714],\n",
       "        [11.67507568, 53.32492432],\n",
       "        [11.49545913, 52.50454087],\n",
       "        [13.65085772, 62.34914228],\n",
       "        [18.68012109, 85.31987891],\n",
       "        [13.83047427, 63.16952573],\n",
       "        [12.57315843, 57.42684157]]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scipy.stats as stats\n",
    "\n",
    "crosstab = pd.crosstab(df[\"region\"], df[\"gender\"])\n",
    "stats.chi2_contingency(crosstab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"3\" halign=\"left\">gender</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gender</th>\n",
       "      <th>female</th>\n",
       "      <th>male</th>\n",
       "      <th>All</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>region</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>africa</th>\n",
       "      <td>0.10</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>asia</th>\n",
       "      <td>0.20</td>\n",
       "      <td>1.11</td>\n",
       "      <td>1.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>australia</th>\n",
       "      <td>0.61</td>\n",
       "      <td>4.74</td>\n",
       "      <td>5.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>canada east</th>\n",
       "      <td>0.81</td>\n",
       "      <td>2.52</td>\n",
       "      <td>3.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>canada west</th>\n",
       "      <td>1.21</td>\n",
       "      <td>2.83</td>\n",
       "      <td>4.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>central east</th>\n",
       "      <td>1.11</td>\n",
       "      <td>4.84</td>\n",
       "      <td>5.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>europe</th>\n",
       "      <td>0.00</td>\n",
       "      <td>4.24</td>\n",
       "      <td>4.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>latin america</th>\n",
       "      <td>0.00</td>\n",
       "      <td>1.41</td>\n",
       "      <td>1.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mid atlantic</th>\n",
       "      <td>1.51</td>\n",
       "      <td>6.26</td>\n",
       "      <td>7.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>north central</th>\n",
       "      <td>2.32</td>\n",
       "      <td>8.68</td>\n",
       "      <td>11.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>north east</th>\n",
       "      <td>1.41</td>\n",
       "      <td>7.57</td>\n",
       "      <td>8.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>north west</th>\n",
       "      <td>1.11</td>\n",
       "      <td>5.45</td>\n",
       "      <td>6.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>northern california</th>\n",
       "      <td>1.11</td>\n",
       "      <td>5.35</td>\n",
       "      <td>6.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>south central</th>\n",
       "      <td>1.01</td>\n",
       "      <td>6.66</td>\n",
       "      <td>7.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>south east</th>\n",
       "      <td>1.92</td>\n",
       "      <td>8.58</td>\n",
       "      <td>10.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>south west</th>\n",
       "      <td>2.02</td>\n",
       "      <td>5.75</td>\n",
       "      <td>7.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>southern california</th>\n",
       "      <td>1.51</td>\n",
       "      <td>5.55</td>\n",
       "      <td>7.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All</th>\n",
       "      <td>17.96</td>\n",
       "      <td>82.04</td>\n",
       "      <td>100.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    gender               \n",
       "gender              female male    All   \n",
       "region                                   \n",
       "africa                0.10   0.50    0.61\n",
       "asia                  0.20   1.11    1.31\n",
       "australia             0.61   4.74    5.35\n",
       "canada east           0.81   2.52    3.33\n",
       "canada west           1.21   2.83    4.04\n",
       "central east          1.11   4.84    5.95\n",
       "europe                0.00   4.24    4.24\n",
       "latin america         0.00   1.41    1.41\n",
       "mid atlantic          1.51   6.26    7.77\n",
       "north central         2.32   8.68   11.00\n",
       "north east            1.41   7.57    8.98\n",
       "north west            1.11   5.45    6.56\n",
       "northern california   1.11   5.35    6.46\n",
       "south central         1.01   6.66    7.67\n",
       "south east            1.92   8.58   10.49\n",
       "south west            2.02   5.75    7.77\n",
       "southern california   1.51   5.55    7.06\n",
       "All                  17.96  82.04  100.00"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crosstab, test_results, expected = rp.crosstab(df[\"region\"], df[\"gender\"],\n",
    "                                               test=\"chi-square\",\n",
    "                                               expected_freqs=True,\n",
    "                                               prop=\"cell\")\n",
    "\n",
    "crosstab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Chi-square test</th>\n",
       "      <th>results</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Pearson Chi-square ( 16.0) =</td>\n",
       "      <td>25.0948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>p-value =</td>\n",
       "      <td>0.0682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Cramer's V =</td>\n",
       "      <td>0.1591</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Chi-square test  results\n",
       "0  Pearson Chi-square ( 16.0) =   25.0948\n",
       "1                     p-value =    0.0682\n",
       "2                  Cramer's V =    0.1591"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ASSUMPTION CHECK\n",
    "\n",
    "Checking the assumptions for the χ2 test of independence is easy. Let's recall what they are:\n",
    "\n",
    "+ The two samples are independent\n",
    "+ The variables were collected independently of each other, i.e. the answer from one variable was not dependent on the answer of the other\n",
    "+ No expected cell count is = 0\n",
    "+ No more than 20% of the cells have and expected cell count < 5\n",
    "The last two assumptions can be checked by looking at the expected frequency table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"2\" halign=\"left\">gender</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gender</th>\n",
       "      <th>female</th>\n",
       "      <th>male</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>region</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>africa</th>\n",
       "      <td>1.077699</td>\n",
       "      <td>4.922301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>asia</th>\n",
       "      <td>2.335015</td>\n",
       "      <td>10.664985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>australia</th>\n",
       "      <td>9.519677</td>\n",
       "      <td>43.480323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>canada east</th>\n",
       "      <td>5.927346</td>\n",
       "      <td>27.072654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>canada west</th>\n",
       "      <td>7.184662</td>\n",
       "      <td>32.815338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>central east</th>\n",
       "      <td>10.597376</td>\n",
       "      <td>48.402624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>europe</th>\n",
       "      <td>7.543895</td>\n",
       "      <td>34.456105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>latin america</th>\n",
       "      <td>2.514632</td>\n",
       "      <td>11.485368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mid atlantic</th>\n",
       "      <td>13.830474</td>\n",
       "      <td>63.169526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>north central</th>\n",
       "      <td>19.578204</td>\n",
       "      <td>89.421796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>north east</th>\n",
       "      <td>15.985873</td>\n",
       "      <td>73.014127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>north west</th>\n",
       "      <td>11.675076</td>\n",
       "      <td>53.324924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>northern california</th>\n",
       "      <td>11.495459</td>\n",
       "      <td>52.504541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>south central</th>\n",
       "      <td>13.650858</td>\n",
       "      <td>62.349142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>south east</th>\n",
       "      <td>18.680121</td>\n",
       "      <td>85.319879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>south west</th>\n",
       "      <td>13.830474</td>\n",
       "      <td>63.169526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>southern california</th>\n",
       "      <td>12.573158</td>\n",
       "      <td>57.426842</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        gender           \n",
       "gender                  female     male  \n",
       "region                                   \n",
       "africa                1.077699   4.922301\n",
       "asia                  2.335015  10.664985\n",
       "australia             9.519677  43.480323\n",
       "canada east           5.927346  27.072654\n",
       "canada west           7.184662  32.815338\n",
       "central east         10.597376  48.402624\n",
       "europe                7.543895  34.456105\n",
       "latin america         2.514632  11.485368\n",
       "mid atlantic         13.830474  63.169526\n",
       "north central        19.578204  89.421796\n",
       "north east           15.985873  73.014127\n",
       "north west           11.675076  53.324924\n",
       "northern california  11.495459  52.504541\n",
       "south central        13.650858  62.349142\n",
       "south east           18.680121  85.319879\n",
       "south west           13.830474  63.169526\n",
       "southern california  12.573158  57.426842"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[12, 68], [144, 113]]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table = [[12, 68],\n",
    "         [144, 113]]\n",
    "\n",
    "table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[12, 68], [144, 113]]\n",
      "\n",
      "\n",
      "\n",
      "dof=1\n",
      "\n",
      "\n",
      "\n",
      "[[ 37.03264095  42.96735905]\n",
      " [118.96735905 138.03264095]]\n",
      "\n",
      "\n",
      "\n",
      "probability=0.990, critical=6.635, stat=39.678\n",
      "\n",
      "\n",
      "\n",
      "Dependent (reject H0)\n",
      "\n",
      "\n",
      "\n",
      "significance=0.010, p=0.000\n",
      "Dependent (reject H0)\n"
     ]
    }
   ],
   "source": [
    "# chi-squared test with similar proportions\n",
    "from scipy.stats import chi2_contingency\n",
    "from scipy.stats import chi2\n",
    "\n",
    "# contingency table\n",
    "table = table\n",
    "print(table)\n",
    "stat, p, dof, expected = chi2_contingency(table)\n",
    "print('\\n\\n')\n",
    "\n",
    "print('dof=%d' % dof)\n",
    "print('\\n\\n')\n",
    "\n",
    "print(expected)\n",
    "# interpret test-statistic\n",
    "prob = 0.99\n",
    "critical = chi2.ppf(prob, dof)\n",
    "print('\\n\\n')\n",
    "print('probability=%.3f, critical=%.3f, stat=%.3f' % (prob, critical, stat))\n",
    "print('\\n\\n')\n",
    "if abs(stat) >= critical:\n",
    "    print('Dependent (reject H0)')\n",
    "else:\n",
    "    print('Independent (fail to reject H0)')\n",
    "print('\\n\\n')\n",
    "# interpret p-value\n",
    "alpha = 1.0 - prob\n",
    "print('significance=%.3f, p=%.3f' % (alpha, p))\n",
    "if p <= alpha:\n",
    "    print('Dependent (reject H0)')\n",
    "else:\n",
    "    print('Independent (fail to reject H0)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "This section lists some ideas for extending the tutorial that you may wish to explore.\n",
    "\n",
    "Update the chi-squared test to use your own contingency table.\n",
    "Write a function to report on the independence given observations from two categorical variables\n",
    "Load a standard machine learning dataset containing categorical variables and report on the independence of each.\n",
    "\n",
    "Pairs of categorical variables can be summarized using a contingency table.\n",
    "The chi-squared test can compare an observed contingency table to an expected table and determine if the categorical variables are independent.\n",
    "How to calculate and interpret the chi-squared test for categorical variables in Python.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## --------> OBSERVATION\n",
    "The one piece of information that researchpy calculates that scipy.stats does not is a measure of the strength of the relationship - this is akin to a correlation statistic such as Pearson's correlation coefficient. A good peer-reviewed article that is not behind a paywall is written by Akoglu (2018). The following table is reproduced from the mentioned article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook EDA.ipynb to python\n",
      "[NbConvertApp] Writing 33153 bytes to EDA.py\n",
      "[NbConvertApp] Converting notebook preprocessing.ipynb to python\n",
      "[NbConvertApp] Writing 10095 bytes to preprocessing.py\n",
      "[NbConvertApp] Converting notebook quiz_3.ipynb to python\n",
      "[NbConvertApp] Writing 16815 bytes to quiz_3.py\n",
      "[NbConvertApp] Converting notebook normality_test.ipynb to python\n",
      "[NbConvertApp] Writing 9797 bytes to normality_test.py\n",
      "[NbConvertApp] Converting notebook quiz_2.ipynb to python\n",
      "[NbConvertApp] ERROR | Notebook JSON is invalid: data.cells[{data__cells_x}] must be valid exactly by one definition (0 matches found)\n",
      "\n",
      "Failed validating <unset> in notebook['data']['cells']:\n",
      "\n",
      "On instance:\n",
      "<unset>\n",
      "[NbConvertApp] Writing 9235 bytes to quiz_2.py\n",
      "[NbConvertApp] Converting notebook regression.ipynb to python\n",
      "[NbConvertApp] Writing 3510 bytes to regression.py\n",
      "[NbConvertApp] Converting notebook statistical_methods.ipynb to python\n",
      "[NbConvertApp] Writing 3015 bytes to statistical_methods.py\n",
      "[NbConvertApp] Converting notebook demo_normality_test.ipynb to python\n",
      "[NbConvertApp] Writing 3950 bytes to demo_normality_test.py\n",
      "[NbConvertApp] Converting notebook model_drop_cols.ipynb to python\n",
      "[NbConvertApp] Writing 25511 bytes to model_drop_cols.py\n",
      "[NbConvertApp] Converting notebook model.ipynb to python\n",
      "[NbConvertApp] Writing 25334 bytes to model.py\n"
     ]
    }
   ],
   "source": [
    "# The .py format of the jupyter notebook\n",
    "import os\n",
    "\n",
    "for fname in os.listdir():\n",
    "    if fname.endswith('ipynb'):\n",
    "        os.system(f'jupyter nbconvert {fname} --to python')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# References\n",
    "Seabold, Skipper, and Josef Perktold. “statsmodels: Econometric and statistical modeling with python.” Proceedings of the 9th Python in Science Conference. 2010.\n",
    "Virtanen P, Gommers R, Oliphant TE, Haberland M, Reddy T, Cournapeau D, Burovski E, Peterson P, Weckesser W, Bright J, van der Walt SJ. SciPy 1.0: fundamental algorithms for scientific computing in Python. Nature methods. 2020 Mar;17(3):261-72.\n",
    "Mangiafico, S.S. 2015. An R Companion for the Handbook of Biological Statistics, version 1.3.2.\n",
    "Knief U, Forstmeier W. Violating the normality assumption may be the lesser of two evils. bioRxiv. 2018 Jan 1:498931.\n",
    "Kozak M, Piepho HP. What’s normal anyway? Residual plots are more telling than significance tests when checking ANOVA assumptions. Journal of Agronomy and Crop Science. 2018 Feb;204(1):86-98."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
